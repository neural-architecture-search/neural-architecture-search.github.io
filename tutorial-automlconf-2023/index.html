<!doctype html>
<html lang="en">
	<head>
		<meta charset="utf-8">
		<meta name="viewport" content="width=device-width, initial-scale=1">
		<meta name="author" content="Martin Wistuba">
		<meta name="title" content="AutoMLConf 2023 Tutorial on Meta-Learning for Hyperparameter Optimization">
		<meta name="description" content="This tutorial seeks to provide a comprehensive overview of meta-learning strategies for efficient hyperparameter optimization.">
		<meta name="keywords" content="Meta-Learning,Hyperparameter Optimization,AutoML">
		<meta name="robots" content="index, follow">
		<meta name="language" content="English">

		<title>Tutorial on Meta-Learning for Hyperparameter Optimization</title>
		<link href="../bootstrap-5.0.0-beta1-dist/css/bootstrap.min.css" rel="stylesheet">
	</head>
	<body style="padding-top: 5rem">
		<!-- Navbar -->
		<nav class="navbar navbar-expand-md navbar-dark bg-dark fixed-top">
			<div class="container-fluid">
				<div class="collapse navbar-collapse" id="navbarsExampleDefault">
					<ul class="navbar-nav me-auto mb-2 mb-md-0">
						<li class="nav-item">
							<a class="nav-link" href="#abstract">Abstract</a>
						</li>
						<li class="nav-item">
							<a class="nav-link" href="#material">Material</a>
						</li>
						<li class="nav-item">
							<a class="nav-link" href="#presenters">Presenters</a>
						</li>
						<li class="nav-item">
							<a class="nav-link" href="#cite">Citation</a>
						</li>
					</ul>
				</div>
			</div>
		</nav>

		<main class="container">
			<div class="starter-template text-center py-5 px-3" id="abstract">
				<h1>Meta-Learning for Hyperparameter Optimization</h1>
				<p class="lead">A tutorial summarizing the progresses in Meta-Learning for Hyperparameter Optimization <br> presented at the AutoML Conference 2023 (<a href="https://2023.automl.cc">AutoMLConf 2023</a>).</p>
			</div>
			
			<!-- Abstract -->
			<div class="container">
				<div class="row">
					<div class="offset-lg-3 col-lg-6" style="text-align:justify">
						<p>
							Hyperparameter Optimization (HPO) has been recognized as a crucial element in achieving state-of-the-art performance
              across a wide range of machine learning tasks. However, the complexity and compute-intensive nature of HPO makes it
              a challenging feat, often inhibiting its adoption and the successful deployment of ML systems to novel tasks. This
              tutorial proposal delves into meta-learning for HPO. Our focus will be on presenting a comprehensive guide on
              meta-learning strategies which facilitates the transfer of design choices across tasks, thereby reducing the
              computational burden. This tutorial aims to equip participants with the knowledge to implement meta-learning
              strategies for HPO, ultimately enhancing the efficiency and effectiveness of their Machine Learning systems.
              Ideal attendees would be machine learning practitioners, researchers, and enthusiasts interested in leveraging
              meta-learning for hyperparameter optimization.
						</p>
					</div>
				</div>
			</div>
			
			<!-- Material -->
			<div class="row" id="material">
				<div class="offset-lg-2 col-lg-8">
					<p class="h1">Tutorial Material</p>
					<div class="row">
						<div class="col-lg-6">
							TBD<!--Slides: <a href="slides.pdf">Download</a> //-->
						</div>
					</div>
				</div>
			</div>
			<div style="padding:30px;"></div>
			
			<!-- Presenters -->
			<div class="row" id="presenters">
				<div class="offset-lg-2 col-lg-8">
					<p class="h1">Presenters</p>
					<div class="row">
            <div class="col-lg-6">
              <div style="text-align:center;">
                <img src="../img/WistubaMartin.png" class="img-fluid rounded-circle img-thumbnail" alt="Martin Wistuba" width="140" height="140">
                <h4>Martin Wistuba</h4>
              </div>
							<p style="text-align:justify">
                Martin Wistuba is a researcher at Amazon Web Services where he works on automation of hyperparameter optimization and
                Neural Architecture Search. Earlier, he was at  IBM Research, where he developed tools to automate deep learning. He
                received his Ph.D. in Machine Learning from the University of Hildesheim. His research interest includes AutoML, in
                particular the idea of meta-knowledge transfer to speed up Bayesian optimization and Neural Architecture Search.
              </p>
						</div>
						<div class="col-lg-6">
              <div style="text-align:center;">
                <img src="../img/GrabockaJosif.jpg" class="img-fluid rounded-circle img-thumbnail" alt="Josif Grabocka" width="140" height="140">
                <h4>Josif Grabocka</h4>
              </div>
							<p style="text-align:justify">
                Josif Grabocka is an Assistant Professor of Representation Learning at the University of Freiburg, where he heads the
                RELEA research group. His primary interest lies in hyperparameter optimization, time-series mining and tabular data.
              </p>
						</div>
					</div>
				</div>
			</div>
			<div style="padding:30px;"></div>
			
			<!-- Cite -->
			<div class="row" id="cite">
				<div class="offset-lg-2 col-lg-8">
					<p class="h1">Citation</p>
					<div class="row">
						<div class="offset-lg-1 col-lg-12">
							<samp>@misc{meta-learning-tutorial-automlconf2023,<br>
								<span style="margin-left: 20px">author       = {Martin Wistuba and Josif Grabocka},<br></span>
								<span style="margin-left: 20px">title        = {Meta-Learning for Hyperparameter Optimization},<br></span>
								<span style="margin-left: 20px">howpublished = {Tutorial at AutoMLConf 2023},<br></span>
								<span style="margin-left: 20px">year         = {2023},<br></span>
								<span style="margin-left: 20px">url          = {https://neural-architecture-search.github.io/tutorial-automlconf-2023},<br></span>
							}</samp>
						</div>
					</div>
				</div>
			</div>
		</main>
	</body>
</html>